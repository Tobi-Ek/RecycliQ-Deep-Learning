# -*- coding: utf-8 -*-
"""RecycliQ_Experiment_N.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a1U1DzagjyRelRjEkFWiJx2ljVIT5Xv-

![image.png](data:image/png;base64,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)

# **Research Project**
### OLUWATOBI EKUNDAYO - x19173105
#### **MSc. Data Analytics, National College of Ireland**

<h1>Table of Contents<span class="tocSkip"></span></h1>
<div class="toc"><ul class="toc-item">
  <li><span><a href="#Experiment-N:-Test-ML/DL-Models-with-NEW-dataset" data-toc-modified-id="Experiment-N:-Test-ML/DL-Models-with-NEW-dataset-1"><span class="toc-item-num">1.&nbsp;&nbsp;</span>Experiment N: Test ML/DL Models with NEW dataset</a></span></li>
  <li><span><a href="#Experiment-2:-State-of-the-art-Model" data-toc-modified-id="Experiment-2:-State-of-the-art-Model-2"><span class="toc-item-num">2.&nbsp;&nbsp;</span>Experiment 2: State-of-the-art Model</a></span></li>
  <li><span><a href="#Experiment-3:-MobileNetV2-Model" data-toc-modified-id="Experiment-3:-MobileNetV2-Model-3"><span class="toc-item-num">3.&nbsp;&nbsp;</span>Experiment 3: MobiletNetV2 Model</a></span></li>
  <li><span><a href="#Experiment-4:-VGG19-Model" data-toc-modified-id="Experiment-4:-VGG19-Model-4"><span class="toc-item-num">4.&nbsp;&nbsp;</span>Experiment 4: VGG19 Model</a></span></li>
  <li><span><a href="#Experiment-5:-DenseNet-Model" data-toc-modified-id="Experiment-5:-DenseNet-Model-5"><span class="toc-item-num">5.&nbsp;&nbsp;</span>Experiment 5: DenseNet Model</a></span></li>
  <li><span><a href="#Experiment-6:-ResNet152-Model" data-toc-modified-id="Experiment-6:-ResNet152-Model-6"><span class="toc-item-num">6.&nbsp;&nbsp;</span>Experiment 6: ResNet152 Model</a></span></li>
  <li><span><a href="#Experiment-7:-Inception-ResNet-V2-Model" data-toc-modified-id="Experiment-7:-Inception-ResNet-V2-Model-7"><span class="toc-item-num">7.&nbsp;&nbsp;</span>Experiment 7: Inception-ResNet-V2 Model</a></span></li>
  <li><span><a href="#Experiment-8:-Xception-Model" data-toc-modified-id="Experiment-8:-Xception-Model-8"><span class="toc-item-num">8.&nbsp;&nbsp;</span>Experiment 8: Xception Model</a></span></li>
  <li><span><a href="#EVALUATION:-Compare-Model-Performance" data-toc-modified-id="EVALUATION:-Compare-Model-Performance-8"><span class="toc-item-num">9.&nbsp;&nbsp;</span>EVALUATION: Compare Model Performance</a></span></li>  
 </ul></div>

## **Experiment N: Test ML/DL Models with NEW dataset**

### **Import Packages**
"""

#Install the package if it hasn't been installed on your colab before.
#!pip install scikit-learn

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import pandas as pd
assert tf.__version__.startswith('2')

import os
import numpy as np
import matplotlib.pyplot as plt

# %matplotlib inline
from scipy import stats
from sklearn.svm import SVC
from sklearn.decomposition import PCA as RandomizedPCA
from sklearn.pipeline import make_pipeline

# use seaborn plotting defaults
import seaborn as sns; sns.set_theme()

tf.__version__

"""### **Setting Up Input Connection To Google Drive**"""

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive 
from google.colab import auth
from oauth2client.client import GoogleCredentials

"""**Get google drive authentication code**"""

auth.authenticate_user()
  gauth = GoogleAuth()
  gauth.credentials = GoogleCredentials.get_application_default()
  drive = GoogleDrive(gauth)

"""**Set path to extract file from gdrive**"""

fid = drive.ListFile({'q':"title='dataset-recycliq.zip'"}).GetList()[0]['id']
f = drive.CreateFile({'id': fid})
f.GetContentFile('trashnet_dataset.zip')

"""**Unzip the file from google drive**"""

!unzip \*.zip  && rm *.zip

"""### **Setup Input Pipeline**
Delete the .txt or .json files from Colab Files before running this code
"""

PATH = "/content/dataset-recycliq"
dirs = os.listdir(PATH)
for file in dirs:
  print (file)

"""**Dataset Summary**"""

from pathlib import Path

ds = len(dirs)
data_dir = Path(PATH)
image_count = len(list(data_dir.glob('*/*.jpg')))
print("There are a total of " + str(image_count) +
      " Images and " + str(ds) + " Classes in this dataset.")

"""### **Preparing the data**

Use **`ImageDataGenerator`** to rescale the images to **train and validation generator**.
"""

IMAGE_SIZE = 224
BATCH_SIZE = 32

datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    rescale=1./255, 
    validation_split=0.2)

train_generator = datagen.flow_from_directory(
    PATH,
    target_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE,
    seed=20,
    subset='training')

val_generator = datagen.flow_from_directory(
    PATH,
    target_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE, 
    seed=20,
    subset='validation')

"""**Extract converted ML Image/Label for the Training and Test data**"""

for image_train, label_train in train_generator:
  break
Xtrain = image_train
ytrain = label_train
# Get Shape
image_train.shape, label_train.shape

for image_test, label_test in val_generator:
  break
Xtest = image_test
ytest = label_test
# Get Shape
image_test.shape, label_test.shape

"""**Visualise the data**"""

sample_training_images, _ = next(train_generator)

def plotImages(images_arr):
    fig, axes = plt.subplots(1, 20, figsize=(10,10))
    axes = axes.flatten()
    for img, ax in zip( images_arr, axes):
        ax.imshow(img)
        ax.axis('off')
    plt.tight_layout
    plt.show()

plotImages(sample_training_images[:20])

"""**Save the labels in a file which will be downloaded later.**"""

print (train_generator.class_indices)
wastelabels = '\n'.join(sorted(train_generator.class_indices.keys()))
with open('wastelabels.txt', 'w') as f:
  f.write(wastelabels)

!cat wastelabels.txt

ab = sorted(train_generator.class_indices.keys())
label_names =np.array(list(ab))
kb = sorted(val_generator.class_indices.keys())
labelval_names =np.array(list(kb))
type(labelval_names)

"""## **Experiment 2: ``State-of-the-art`` Model**

**Reshaping the Train and Test data**
"""

# When using size 224 reshape with (32*6,112*224)
# When using size 256 reshape with (32*6,128*256)

X_train = Xtrain.reshape(32*7,96*224)
X_test = Xtest.reshape(32*7,96*224)

y_train = ytrain.flatten()
y_test = ytest.flatten()

"""**Support Vector Machine (SVM)**"""

#Create a svm Classifier
clf = SVC(kernel='linear') # Linear Kernel

#Train the model using the training sets
clf.fit(X_train, y_train)

#Predict the response for test dataset
y_pred = clf.predict(X_test)

"""**Evaluation Metrics**"""

# Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics

# SVM Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
# SVM Model Precision
print("\nPrecision:",metrics.precision_score(y_test, y_pred))
# SVM Model Recall
print("Recall:",metrics.recall_score(y_test, y_pred))

"""**Converting numerical labels to categorical format**"""

def ToCategory(zz):
    bb=0
    jj=list()
    
    for n in zz:
        if (n == [1., 0., 0., 0., 0., 0., 0.]):
            bb = 0
        if (n == [0., 1., 0., 0., 0., 0., 0.]):
            bb = 1
        if (n == [0., 0., 1., 0., 0., 0., 0.]):
            bb = 2
        if (n == [0., 0., 0., 1., 0., 0., 0.]):
            bb = 3
        if (n == [0., 0., 0., 0., 1., 0., 0.]):
            bb = 4
        if (n == [0., 0., 0., 0., 0., 1., 0.]):
            bb = 5
        if (n == [0., 0., 0., 0., 0., 0., 1.]):
            bb = 6            
        jj.append(bb)
    kk = np.asarray(jj)
    return kk

# Convert test label to category format
cc = ytest.tolist()
yTEST = ToCategory(cc)

yPRED

"""**Display the confusion matrix between the classes**"""

from sklearn.metrics import confusion_matrix

# Convert prediction label to category format
yy = y_pred.reshape(32,7)
dd = yy.tolist()
yPRED = ToCategory(dd)

mat = confusion_matrix(yTEST, yPRED)
sns.heatmap(mat.T, square=True, annot=True, fmt='d', cmap="YlGnBu", cbar=True,
            xticklabels=label_names,
            yticklabels=label_names)
plt.xlabel('true label')
plt.ylabel('predicted label');

"""### **Using PCA (feature extraction) + SVM (classifier) Model**"""

# Set PCA for feature extraction
pca = RandomizedPCA(n_components=150, whiten=True, random_state=42)
# Set SVM for classification
svc = SVC(kernel='rbf', class_weight='balanced')
# Setup the model
svm_model = make_pipeline(pca, svc)

"""Use **``grid search cross-validation``** to explore combinations of parameters.
Here we will adjust **``C``** (which controls the margin hardness) and **``gamma``** (which controls the size of the radial basis function kernel), and determine the best model:
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.model_selection import GridSearchCV
param_grid = {'svc__C': [1, 5, 10, 50, 100],
              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01]}

grid = GridSearchCV(svm_model, param_grid, cv=5)

# %time grid.fit(X_train, y_train)
print(grid.best_params_)

"""**``The optimal values from the grid search falls toward the middle of the grid``**

**Using the cross-validated model** - Predict the labels for the test data, which the model has not yet seen.
"""

svm_model = grid.best_estimator_
yfit = svm_model.predict(X_test)

"""**Evaluation Metrics**"""

# Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics

# Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, yfit))

"""**Using the classification report we evaluate the estimator's performance**"""

from sklearn.metrics import classification_report

# Convert prediction label to category format
gg = yfit.reshape(32,7)
pp = gg.tolist()
yFIT = ToCategory(pp)

print(classification_report(yTEST, yFIT, target_names=label_names))

"""**Display the confusion matrix between the classes**"""

from sklearn.metrics import confusion_matrix
mat = confusion_matrix(yTEST, yFIT)
sns.heatmap(mat.T, square=True, annot=True, fmt='d', cmap="YlGnBu", cbar=True,
            xticklabels=label_names,
            yticklabels=label_names)
plt.xlabel('true label')
plt.ylabel('predicted label');

"""### Summary:

* **Model**: State-of-the-art Model (SVM)
* **Size**:
* **Loss**: 
* **Latency**:
* **Training Time**:

## **Experiment 3: ``MobileNetV2`` Model**

### **Create base model from the pre-trained convnets**

We instantiate a **`MobileNetV2`** model pre-loaded with weights trained on ImageNet. By specifying the **`include_top=False`** argument, we load a network that doesn't include the classification layers at the top, which is ideal for feature extraction.
"""

IMG_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)

base_model_3 = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                              include_top=False, 
                                              weights='imagenet')

print("\n Your selected Model has an input size of {} ".format(IMG_SHAPE))

"""### **Feature extraction**

Freeze the convolutional base created from the previous step and use that as a feature extractor, add a classifier on top of it and train the top-level classifier.
"""

base_model_3.trainable = False

"""**Add a classification head**"""

mnv2_model = tf.keras.Sequential([
  base_model_3,
  tf.keras.layers.Conv2D(32, 3, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.GlobalAveragePooling2D(),
  tf.keras.layers.Dense(7, activation='softmax')
])

"""**Compile the model**

You must compile the model before training it.  Since there are multiple classes, use a categorical cross-entropy loss.
"""

mnv2_model.compile(optimizer=tf.keras.optimizers.Adam(), 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

mnv2_model.summary()

print('Number of trainable variables = {}'.format(len(mnv2_model.trainable_variables)))

"""### **Train the model**

<!-- TODO(markdaoust): delete steps_per_epoch in TensorFlow r1.14/r2.0 -->
"""

epochs = 10
history_3 = mnv2_model.fit(train_generator, 
                    steps_per_epoch=len(train_generator), 
                    epochs=epochs, 
                    validation_data=val_generator, 
                    validation_steps=len(val_generator))

"""**Plot learning curves of the training and validation accuracy/loss**

Let's take a look at the learning curves of the training and validation accuracy/loss when using the base model as a fixed feature extractor.
"""

acc = history_3.history['accuracy']
val_acc = history_3.history['val_accuracy']

loss = history_3.history['loss']
val_loss = history_3.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

"""### **Fine tuning**

To increase performance we'll train (or "fine-tune") the weights of the top layers of the pre-trained model alongside the training of the added classifier. The training process will force the weights to be tuned from generic features maps to features associated specifically to our dataset.

**Un-freeze the top layers of the model**

Unfreeze the `base_model` and set the bottom layers be un-trainable. Then, recompile the model (necessary for these changes to take effect), and resume training.
"""

base_model_3.trainable = True

# Let's take a look to see how many layers are in the base model
print("Number of layers in the base model: ", len(base_model_3.layers))

# Fine tune from this layer onwards
fine_tune_at = 100

# Freeze all the layers before the `fine_tune_at` layer
for layer in base_model_3.layers[:fine_tune_at]:
  layer.trainable =  False

"""**Compile the model** - using a much lower training rate."""

mnv2_model.compile(loss='categorical_crossentropy',
              optimizer = tf.keras.optimizers.Adam(1e-5),
              metrics=['accuracy'])

"""**Model Summary**"""

mnv2_model.summary()

print('Number of trainable variables = {}'.format(len(mnv2_model.trainable_variables)))

"""### **Complete model training**"""

history_fine3 = mnv2_model.fit(train_generator, 
                         steps_per_epoch=len(train_generator), 
                         epochs=10, 
                         validation_data=val_generator, 
                         validation_steps=len(val_generator))

"""**Plot learning curves of the training and validation accuracy/loss**"""

acc = history_fine3.history['accuracy']
val_acc = history_fine3.history['val_accuracy']

loss = history_fine3.history['loss']
val_loss = history_fine3.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

"""### **Convert to TFLite**

Saved the model using **`tf.saved_model.save`**
"""

saved_model_dir = 'save/fine_tuning'
tf.saved_model.save(mnv2_model, saved_model_dir)

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)

"""**Post-Training Quantization**

This technique is enabled as an option in the TensorFlow Lite converter to reduce the precision of the numbers in weight and biases in the model.

The simplest form of post-training quantization quantizes weights from floating point to 8-bits of precision. At inference, weights are converted from 8-bits of precision to floating point and computed using floating-point kernels. This conversion is done once and cached to reduce latency.

To further improve latency, hybrid operators dynamically quantize activations to 8-bits and perform computations with 8-bit weights and activations. This optimization provides latencies close to fully fixed-point inference. However, the outputs are still stored using floating point, so that the speedup with hybrid ops is less than a full fixed-point computation.
"""

converter.optimizations = [tf.lite.Optimize.DEFAULT]

"""**Convert the saved model to a TF lite compatible format**"""

tflite_model_3 = converter.convert()

with open('mnv2_model.tflite', 'wb') as f:
  f.write(tflite_model_3)

"""**Download the converted model and labels**"""

from google.colab import files

files.download('mnv2_model.tflite')
files.download('wastelabels.txt')

"""### Summary:

* **Model**: MobileNetV2 Model
* **Size**:
* **Loss**: 
* **Latency**:
* **Training Time**:

## **Experiment 4: ``VGG19`` Model**

### **Create base model from the pre-trained convnets**

We instantiate a **`VGG19`** model pre-loaded with weights trained on ImageNet. By specifying the **`include_top=False`** argument, we load a network that doesn't include the classification layers at the top, which is ideal for feature extraction.
"""

IMG_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)

base_model_4 = tf.keras.applications.VGG19(input_shape=IMG_SHAPE,
                                              include_top=False, 
                                              weights='imagenet')

print("\n Your selected Model has an input size of {} ".format(IMG_SHAPE))

"""### **Feature extraction**

Freeze the convolutional base created from the previous step and use that as a feature extractor, add a classifier on top of it and train the top-level classifier.
"""

base_model_4.trainable = False

"""**Add a classification head**"""

vgg_model = tf.keras.Sequential([
  base_model_4,
  tf.keras.layers.Conv2D(32, 3, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.GlobalAveragePooling2D(),
  tf.keras.layers.Dense(7, activation='softmax')
])

"""**Compile the model**

You must compile the model before training it.  Since there are multiple classes, use a categorical cross-entropy loss.
"""

vgg_model.compile(optimizer=tf.keras.optimizers.Adam(), 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

vgg_model.summary()

print('Number of trainable variables = {}'.format(len(vgg_model.trainable_variables)))

"""### **Train the model**

<!-- TODO(markdaoust): delete steps_per_epoch in TensorFlow r1.14/r2.0 -->
"""

epochs = 10

history_4 = vgg_model.fit(train_generator, 
                    steps_per_epoch=len(train_generator), 
                    epochs=epochs, 
                    validation_data=val_generator, 
                    validation_steps=len(val_generator))

"""**Plot learning curves of the training and validation accuracy/loss**

Let's take a look at the learning curves of the training and validation accuracy/loss when using the base model as a fixed feature extractor.
"""

acc = history_4.history['accuracy']
val_acc = history_4.history['val_accuracy']

loss = history_4.history['loss']
val_loss = history_4.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

"""### **Fine tuning**

To increase performance we'll train (or "fine-tune") the weights of the top layers of the pre-trained model alongside the training of the added classifier. The training process will force the weights to be tuned from generic features maps to features associated specifically to our dataset.

**Un-freeze the top layers of the model**

Unfreeze the `base_model` and set the bottom layers be un-trainable. Then, recompile the model (necessary for these changes to take effect), and resume training.
"""

base_model_4.trainable = True

# Let's take a look to see how many layers are in the base model
print("Number of layers in the base model: ", len(base_model_4.layers))

# Fine tune from this layer onwards
fine_tune_at = 100

# Freeze all the layers before the `fine_tune_at` layer
for layer in base_model_4.layers[:fine_tune_at]:
  layer.trainable =  False

"""**Compile the model** - using a much lower training rate."""

vgg_model.compile(loss='categorical_crossentropy',
              optimizer = tf.keras.optimizers.Adam(1e-5),
              metrics=['accuracy'])

"""**Model Summary**"""

vgg_model.summary()

print('Number of trainable variables = {}'.format(len(vgg_model.trainable_variables)))

"""### **Complete model training**"""

history_fine_4 = vgg_model.fit(train_generator, 
                         steps_per_epoch=len(train_generator), 
                         epochs=10, 
                         validation_data=val_generator, 
                         validation_steps=len(val_generator))

"""**Plot learning curves of the training and validation accuracy/loss**"""

acc = history_fine_4.history['accuracy']
val_acc = history_fine_4.history['val_accuracy']

loss = history_fine_4.history['loss']
val_loss = history_fine_4.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

"""### **Convert to TFLite**

Saved the model using **`tf.saved_model.save`**
"""

saved_model_dir = 'save/fine_tuning'
tf.saved_model.save(vgg_model, saved_model_dir)

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)

"""**Post-Training Quantization**

This technique is enabled as an option in the TensorFlow Lite converter to reduce the precision of the numbers in weight and biases in the model.

The simplest form of post-training quantization quantizes weights from floating point to 8-bits of precision. At inference, weights are converted from 8-bits of precision to floating point and computed using floating-point kernels. This conversion is done once and cached to reduce latency.

To further improve latency, hybrid operators dynamically quantize activations to 8-bits and perform computations with 8-bit weights and activations. This optimization provides latencies close to fully fixed-point inference. However, the outputs are still stored using floating point, so that the speedup with hybrid ops is less than a full fixed-point computation.
"""

converter.optimizations = [tf.lite.Optimize.DEFAULT]

"""**Convert the saved model to a TF lite compatible format**"""

tflite_model_4 = converter.convert()

with open('vgg_model.tflite', 'wb') as f:
  f.write(tflite_model_4)

"""**Download the converted model and labels**"""

from google.colab import files

files.download('vgg_model.tflite')
#files.download('wastelabels.txt')

"""### Summary:

* **Model**: VGG19 Model
* **Size**: 19 MB
* **Loss**: 
* **Latency**:
* **Training Time**:

## **Experiment 5: ``DenseNet`` Model**

### **Create base model from the pre-trained convnets**

We instantiate a **`DenseNet`** model pre-loaded with weights trained on ImageNet. By specifying the **`include_top=False`** argument, we load a network that doesn't include the classification layers at the top, which is ideal for feature extraction.
"""

IMG_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)

base_model_5 = tf.keras.applications.DenseNet201(input_shape=IMG_SHAPE,
                                              include_top=False, 
                                              weights='imagenet')

print("\n Your selected Model has an input size of {} ".format(IMG_SHAPE))

"""### **Feature extraction**

Freeze the convolutional base created from the previous step and use that as a feature extractor, add a classifier on top of it and train the top-level classifier.
"""

base_model_5.trainable = False

"""**Add a classification head**"""

dsn_model = tf.keras.Sequential([
  base_model_5,
  tf.keras.layers.Conv2D(32, 3, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.GlobalAveragePooling2D(),
  tf.keras.layers.Dense(7, activation='softmax')
])

"""**Compile the model**

You must compile the model before training it.  Since there are multiple classes, use a categorical cross-entropy loss.
"""

dsn_model.compile(optimizer=tf.keras.optimizers.Adam(), 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

dsn_model.summary()

print('Number of trainable variables = {}'.format(len(dsn_model.trainable_variables)))

"""### **Train the model**

<!-- TODO(markdaoust): delete steps_per_epoch in TensorFlow r1.14/r2.0 -->
"""

epochs = 10

history_5 = dsn_model.fit(train_generator, 
                    steps_per_epoch=len(train_generator), 
                    epochs=epochs, 
                    validation_data=val_generator, 
                    validation_steps=len(val_generator))

"""**Plot learning curves of the training and validation accuracy/loss**

Let's take a look at the learning curves of the training and validation accuracy/loss when using the base model as a fixed feature extractor.
"""

acc = history_5.history['accuracy']
val_acc = history_5.history['val_accuracy']

loss = history_5.history['loss']
val_loss = history_5.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

"""### **Fine tuning**

To increase performance we'll train (or "fine-tune") the weights of the top layers of the pre-trained model alongside the training of the added classifier. The training process will force the weights to be tuned from generic features maps to features associated specifically to our dataset.

**Un-freeze the top layers of the model**

Unfreeze the `base_model` and set the bottom layers be un-trainable. Then, recompile the model (necessary for these changes to take effect), and resume training.
"""

base_model_5.trainable = True

# Let's take a look to see how many layers are in the base model
print("Number of layers in the base model: ", len(base_model_5.layers))

# Fine tune from this layer onwards
fine_tune_at = 100

# Freeze all the layers before the `fine_tune_at` layer
for layer in base_model_5.layers[:fine_tune_at]:
  layer.trainable =  False

"""**Compile the model** - using a much lower training rate."""

dsn_model.compile(loss='categorical_crossentropy',
              optimizer = tf.keras.optimizers.Adam(1e-5),
              metrics=['accuracy'])

"""**Model Summary**"""

dsn_model.summary()

print('Number of trainable variables = {}'.format(len(dsn_model.trainable_variables)))

"""### **Complete model training**"""

history_fine_5 = dsn_model.fit(train_generator, 
                         steps_per_epoch=len(train_generator), 
                         epochs=10, 
                         validation_data=val_generator, 
                         validation_steps=len(val_generator))

"""**Plot learning curves of the training and validation accuracy/loss**"""

acc = history_fine_5.history['accuracy']
val_acc = history_fine_5.history['val_accuracy']

loss = history_fine_5.history['loss']
val_loss = history_fine_5.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

"""### **Convert to TFLite**

Saved the model using **`tf.saved_model.save`**
"""

saved_model_dir = 'save/fine_tuning'
tf.saved_model.save(dsn_model, saved_model_dir)

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)

"""**Post-Training Quantization**

This technique is enabled as an option in the TensorFlow Lite converter to reduce the precision of the numbers in weight and biases in the model.

The simplest form of post-training quantization quantizes weights from floating point to 8-bits of precision. At inference, weights are converted from 8-bits of precision to floating point and computed using floating-point kernels. This conversion is done once and cached to reduce latency.

To further improve latency, hybrid operators dynamically quantize activations to 8-bits and perform computations with 8-bit weights and activations. This optimization provides latencies close to fully fixed-point inference. However, the outputs are still stored using floating point, so that the speedup with hybrid ops is less than a full fixed-point computation.
"""

converter.optimizations = [tf.lite.Optimize.DEFAULT]

"""**Convert the saved model to a TF lite compatible format**"""

tflite_model_5 = converter.convert()

with open('dsn_model.tflite', 'wb') as f:
  f.write(tflite_model_5)

"""**Download the converted model and labels**"""

from google.colab import files

files.download('dsn_model.tflite')
#files.download('wastelabels.txt')

"""### Summary:

* **Model**: DenseNet Model
* **Size**:
* **Loss**: 
* **Latency**:
* **Training Time**:

## **Experiment 6: ``ResNet152`` Model**

### **Create base model from the pre-trained convnets**

We instantiate a **`ResNet152`** model pre-loaded with weights trained on ImageNet. By specifying the **`include_top=False`** argument, we load a network that doesn't include the classification layers at the top, which is ideal for feature extraction.
"""

IMG_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)

base_model_6 = tf.keras.applications.ResNet152(input_shape=IMG_SHAPE,
                                              include_top=False, 
                                              weights='imagenet')

print("\n Your selected Model has an input size of {} ".format(IMG_SHAPE))

"""### **Feature extraction**

Freeze the convolutional base created from the previous step and use that as a feature extractor, add a classifier on top of it and train the top-level classifier.
"""

base_model_6.trainable = False

"""**Add a classification head**"""

rsn152_model = tf.keras.Sequential([
  base_model_6,
  tf.keras.layers.Conv2D(32, 3, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.GlobalAveragePooling2D(),
  tf.keras.layers.Dense(7, activation='softmax')
])

"""**Compile the model**

You must compile the model before training it.  Since there are multiple classes, use a categorical cross-entropy loss.
"""

rsn152_model.compile(optimizer=tf.keras.optimizers.Adam(), 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

rsn152_model.summary()

print('Number of trainable variables = {}'.format(len(rsn152_model.trainable_variables)))

"""### **Train the model**

<!-- TODO(markdaoust): delete steps_per_epoch in TensorFlow r1.14/r2.0 -->
"""

epochs = 10

history_6 = rsn152_model.fit(train_generator, 
                    steps_per_epoch=len(train_generator), 
                    epochs=epochs, 
                    validation_data=val_generator, 
                    validation_steps=len(val_generator))

"""**Plot learning curves of the training and validation accuracy/loss**

Let's take a look at the learning curves of the training and validation accuracy/loss when using the base model as a fixed feature extractor.
"""

acc = history_6.history['accuracy']
val_acc = history_6.history['val_accuracy']

loss = history_6.history['loss']
val_loss = history_6.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

"""### **Fine tuning**

To increase performance we'll train (or "fine-tune") the weights of the top layers of the pre-trained model alongside the training of the added classifier. The training process will force the weights to be tuned from generic features maps to features associated specifically to our dataset.

**Un-freeze the top layers of the model**

Unfreeze the `base_model` and set the bottom layers be un-trainable. Then, recompile the model (necessary for these changes to take effect), and resume training.
"""

base_model_6.trainable = True

# Let's take a look to see how many layers are in the base model
print("Number of layers in the base model: ", len(base_model_6.layers))

# Fine tune from this layer onwards
fine_tune_at = 100

# Freeze all the layers before the `fine_tune_at` layer
for layer in base_model_6.layers[:fine_tune_at]:
  layer.trainable =  False

"""**Compile the model** - using a much lower training rate."""

rsn152_model.compile(loss='categorical_crossentropy',
              optimizer = tf.keras.optimizers.Adam(1e-5),
              metrics=['accuracy'])

"""**Model Summary**"""

rsn152_model.summary()

print('Number of trainable variables = {}'.format(len(rsn152_model.trainable_variables)))

"""### **Complete model training**"""

history_fine_6 = rsn152_model.fit(train_generator, 
                         steps_per_epoch=len(train_generator), 
                         epochs=10, 
                         validation_data=val_generator, 
                         validation_steps=len(val_generator))

"""**Plot learning curves of the training and validation accuracy/loss**"""

acc = history_fine_6.history['accuracy']
val_acc = history_fine_6.history['val_accuracy']

loss = history_fine_6.history['loss']
val_loss = history_fine_6.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

"""### **Convert to TFLite**

Saved the model using **`tf.saved_model.save`**
"""

saved_model_dir = 'save/fine_tuning'
tf.saved_model.save(rsn152_model, saved_model_dir)

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)

"""**Post-Training Quantization**

This technique is enabled as an option in the TensorFlow Lite converter to reduce the precision of the numbers in weight and biases in the model.

The simplest form of post-training quantization quantizes weights from floating point to 8-bits of precision. At inference, weights are converted from 8-bits of precision to floating point and computed using floating-point kernels. This conversion is done once and cached to reduce latency.

To further improve latency, hybrid operators dynamically quantize activations to 8-bits and perform computations with 8-bit weights and activations. This optimization provides latencies close to fully fixed-point inference. However, the outputs are still stored using floating point, so that the speedup with hybrid ops is less than a full fixed-point computation.
"""

converter.optimizations = [tf.lite.Optimize.DEFAULT]

"""**Convert the saved model to a TF lite compatible format**"""

tflite_model_6 = converter.convert()

with open('rsn152_model.tflite', 'wb') as f:
  f.write(tflite_model_6)

"""**Download the converted model and labels**"""

from google.colab import files

files.download('rsn152_model.tflite')
#files.download('wastelabels.txt')

"""### Summary:

* **Model**: ResNet152 Model
* **Size**:
* **Loss**: 
* **Latency**:
* **Training Time**:

## **Experiment 7: ``Inception-ResNet-V2`` Model**

**NOTE: Input shape for `Inception-ResNet-V2` must be (299, 299, 3)**

### **Create base model from the pre-trained convnets**

We instantiate a **`Inception-ResNet-V2`** model pre-loaded with weights trained on ImageNet. By specifying the **`include_top=False`** argument, we load a network that doesn't include the classification layers at the top, which is ideal for feature extraction.
"""

IMG_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)

base_model_7 = tf.keras.applications.InceptionResNetV2(input_shape=IMG_SHAPE,
                                              include_top=False, 
                                              weights='imagenet')

print("\n Your selected Model has an input size of {} ".format(IMG_SHAPE))

"""### **Feature extraction**

Freeze the convolutional base created from the previous step and use that as a feature extractor, add a classifier on top of it and train the top-level classifier.
"""

base_model_7.trainable = False

"""**Add a classification head**"""

irsnv2_model = tf.keras.Sequential([
  base_model_7,
  tf.keras.layers.Conv2D(32, 3, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.GlobalAveragePooling2D(),
  tf.keras.layers.Dense(7, activation='softmax')
])

"""**Compile the model**

You must compile the model before training it.  Since there are multiple classes, use a categorical cross-entropy loss.
"""

irsnv2_model.compile(optimizer=tf.keras.optimizers.Adam(), 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

irsnv2_model.summary()

print('Number of trainable variables = {}'.format(len(irsnv2_model.trainable_variables)))

"""### **Train the model**

<!-- TODO(markdaoust): delete steps_per_epoch in TensorFlow r1.14/r2.0 -->
"""

epochs = 10

history_7 = irsnv2_model.fit(train_generator, 
                    steps_per_epoch=len(train_generator), 
                    epochs=epochs, 
                    validation_data=val_generator, 
                    validation_steps=len(val_generator))

"""**Plot learning curves of the training and validation accuracy/loss**

Let's take a look at the learning curves of the training and validation accuracy/loss when using the base model as a fixed feature extractor.
"""

acc = history_7.history['accuracy']
val_acc = history_7.history['val_accuracy']

loss = history_7.history['loss']
val_loss = history_7.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

"""### **Fine tuning**

To increase performance we'll train (or "fine-tune") the weights of the top layers of the pre-trained model alongside the training of the added classifier. The training process will force the weights to be tuned from generic features maps to features associated specifically to our dataset.

**Un-freeze the top layers of the model**

Unfreeze the `base_model` and set the bottom layers be un-trainable. Then, recompile the model (necessary for these changes to take effect), and resume training.
"""

base_model_7.trainable = True

# Let's take a look to see how many layers are in the base model
print("Number of layers in the base model: ", len(base_model_7.layers))

# Fine tune from this layer onwards
fine_tune_at = 100

# Freeze all the layers before the `fine_tune_at` layer
for layer in base_model_7.layers[:fine_tune_at]:
  layer.trainable =  False

"""**Compile the model** - using a much lower training rate."""

irsnv2_model.compile(loss='categorical_crossentropy',
              optimizer = tf.keras.optimizers.Adam(1e-5),
              metrics=['accuracy'])

"""**Model Summary**"""

irsnv2_model.summary()

print('Number of trainable variables = {}'.format(len(irsnv2_model.trainable_variables)))

"""### **Complete model training**"""

history_fine_7 = irsnv2_model.fit(train_generator, 
                         steps_per_epoch=len(train_generator), 
                         epochs=10, 
                         validation_data=val_generator, 
                         validation_steps=len(val_generator))

"""**Plot learning curves of the training and validation accuracy/loss**"""

acc = history_fine_7.history['accuracy']
val_acc = history_fine_7.history['val_accuracy']

loss = history_fine_7.history['loss']
val_loss = history_fine_7.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

"""### **Convert to TFLite**

Saved the model using **`tf.saved_model.save`**
"""

saved_model_dir = 'save/fine_tuning'
tf.saved_model.save(irsnv2_model, saved_model_dir)

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)

"""**Post-Training Quantization**

This technique is enabled as an option in the TensorFlow Lite converter to reduce the precision of the numbers in weight and biases in the model.

The simplest form of post-training quantization quantizes weights from floating point to 8-bits of precision. At inference, weights are converted from 8-bits of precision to floating point and computed using floating-point kernels. This conversion is done once and cached to reduce latency.

To further improve latency, hybrid operators dynamically quantize activations to 8-bits and perform computations with 8-bit weights and activations. This optimization provides latencies close to fully fixed-point inference. However, the outputs are still stored using floating point, so that the speedup with hybrid ops is less than a full fixed-point computation.
"""

converter.optimizations = [tf.lite.Optimize.DEFAULT]

"""**Convert the saved model to a TF lite compatible format**"""

tflite_model_7 = converter.convert()

with open('irsnv2_model.tflite', 'wb') as f:
  f.write(tflite_model_7)

"""**Download the converted model and labels**"""

from google.colab import files

files.download('irsnv2_model.tflite')
#files.download('wastelabels.txt')

"""### Summary:

* **Model**: Inception-Resnet-V2 Model
* **Size**: 52.73 MB
* **Loss**: 
* **Latency**:
* **Training Time**:

## **Experiment 8: ``Xception`` Model**

**NOTE: Input shape for `Xception` must be (299, 299, 3)**

### **Create base model from the pre-trained convnets**

We instantiate a **`Inception-ResNet-V2`** model pre-loaded with weights trained on ImageNet. By specifying the **`include_top=False`** argument, we load a network that doesn't include the classification layers at the top, which is ideal for feature extraction.
"""

IMG_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)

base_model_8 = tf.keras.applications.Xception(input_shape=IMG_SHAPE,
                                              include_top=False, 
                                              weights='imagenet')

print("\n Your selected Model has an input size of {} ".format(IMG_SHAPE))

"""### **Feature extraction**

Freeze the convolutional base created from the previous step and use that as a feature extractor, add a classifier on top of it and train the top-level classifier.
"""

base_model_8.trainable = False

"""**Add a classification head**"""

xcep_model = tf.keras.Sequential([
  base_model_8,
  tf.keras.layers.Conv2D(32, 3, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.GlobalAveragePooling2D(),
  tf.keras.layers.Dense(7, activation='softmax')
])

"""**Compile the model**

You must compile the model before training it.  Since there are multiple classes, use a categorical cross-entropy loss.
"""

xcep_model.compile(optimizer=tf.keras.optimizers.Adam(), 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

xcep_model.summary()

print('Number of trainable variables = {}'.format(len(xcep_model.trainable_variables)))

"""### **Train the model**

<!-- TODO(markdaoust): delete steps_per_epoch in TensorFlow r1.14/r2.0 -->
"""

epochs = 10

history_8 = xcep_model.fit(train_generator, 
                    steps_per_epoch=len(train_generator), 
                    epochs=epochs, 
                    validation_data=val_generator, 
                    validation_steps=len(val_generator))

"""**Plot learning curves of the training and validation accuracy/loss**

Let's take a look at the learning curves of the training and validation accuracy/loss when using the base model as a fixed feature extractor.
"""

acc = history_8.history['accuracy']
val_acc = history_8.history['val_accuracy']

loss = history_8.history['loss']
val_loss = history_8.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

"""### **Fine tuning**

To increase performance we'll train (or "fine-tune") the weights of the top layers of the pre-trained model alongside the training of the added classifier. The training process will force the weights to be tuned from generic features maps to features associated specifically to our dataset.

**Un-freeze the top layers of the model**

Unfreeze the `base_model` and set the bottom layers be un-trainable. Then, recompile the model (necessary for these changes to take effect), and resume training.
"""

base_model_8.trainable = True

# Let's take a look to see how many layers are in the base model
print("Number of layers in the base model: ", len(base_model_8.layers))

# Fine tune from this layer onwards
fine_tune_at = 100

# Freeze all the layers before the `fine_tune_at` layer
for layer in base_model_8.layers[:fine_tune_at]:
  layer.trainable =  False

"""**Compile the model** - using a much lower training rate."""

xcep_model.compile(loss='categorical_crossentropy',
              optimizer = tf.keras.optimizers.Adam(1e-5),
              metrics=['accuracy'])

"""**Model Summary**"""

irsnv2_model.summary()

print('Number of trainable variables = {}'.format(len(xcep_model.trainable_variables)))

"""### **Complete model training**"""

history_fine_8 = xcep_model.fit(train_generator, 
                         steps_per_epoch=len(train_generator), 
                         epochs=10, 
                         validation_data=val_generator, 
                         validation_steps=len(val_generator))

"""**Plot learning curves of the training and validation accuracy/loss**"""

acc = history_fine_8.history['accuracy']
val_acc = history_fine_8.history['val_accuracy']

loss = history_fine_8.history['loss']
val_loss = history_fine_8.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='upper left')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper left')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

"""### **Convert to TFLite**

Saved the model using **`tf.saved_model.save`**
"""

saved_model_dir = 'save/fine_tuning'
tf.saved_model.save(xcep_model, saved_model_dir)

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)

"""**Post-Training Quantization**

This technique is enabled as an option in the TensorFlow Lite converter to reduce the precision of the numbers in weight and biases in the model.

The simplest form of post-training quantization quantizes weights from floating point to 8-bits of precision. At inference, weights are converted from 8-bits of precision to floating point and computed using floating-point kernels. This conversion is done once and cached to reduce latency.

To further improve latency, hybrid operators dynamically quantize activations to 8-bits and perform computations with 8-bit weights and activations. This optimization provides latencies close to fully fixed-point inference. However, the outputs are still stored using floating point, so that the speedup with hybrid ops is less than a full fixed-point computation.
"""

converter.optimizations = [tf.lite.Optimize.DEFAULT]

"""**Convert the saved model to a TF lite compatible format**"""

tflite_model_8 = converter.convert()

with open('xcep_model.tflite', 'wb') as f:
  f.write(tflite_model_8)

"""**Download the converted model and labels**"""

from google.colab import files

files.download('xcep_model.tflite')
#files.download('wastelabels.txt')

"""### Summary:

* **Model**: Xception Model
* **Size**: 20.56 MB
* **Loss**: 
* **Latency**:
* **Training Time**:

## **EVALUATION: Compare Model Performance**

### **All trained models**
"""

#history_fine

#svm_model
#mnv2_model
#efn_model
#dsn_model
#rsn152_model
#irsnv2_model

"""**Plot learning curves of the training and validation accuracy/loss**"""

acc = history_fine.history['accuracy']
val_acc = history_fine.history['val_accuracy']

loss = history_fine.history['loss']
val_loss = history_fine.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

"""### **Export model performance**"""

with open('irsnv2_model.tflite', 'wb') as f:
  f.write(tflite_model_7)

"""**Download the files**"""

from google.colab import files

files.download('modelperformance.xls')
files.download('modelperformance.jpeg')

"""### Summary:

* **Using a pre-trained model for feature extraction**:  When working with a small dataset, it is common to take advantage of features learned by a model trained on a larger dataset in the same domain. This is done by instantiating the pre-trained model and adding a fully-connected classifier on top. The pre-trained model is "frozen" and only the weights of the classifier get updated during training.
In this case, the convolutional base extracted all the features associated with each image and you just trained a classifier that determines the image class given that set of extracted features.

* **Fine-tuning a pre-trained model**: To further improve performance, one might want to repurpose the top-level layers of the pre-trained models to the new dataset via fine-tuning.
In this case, you tuned your weights such that your model learned high-level features specific to the dataset. This technique is usually recommended when the training dataset is large and very similar to the orginial dataset that the pre-trained model was trained on.
"""